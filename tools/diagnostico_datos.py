"""
Script: tools/diagnostico_datos.py
Descripci√≥n: Diagn√≥stico completo de los datos limpios antes del modelado GP.
             Identifica problemas comunes que causan R¬≤ negativo.
             
Uso:
    python -m tools.diagnostico_datos
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from config.settings import CONFIG


def diagnosticar_datos():
    """Ejecuta diagn√≥stico completo del dataset limpio."""
    
    print("üî¨ DIAGN√ìSTICO DE DATOS PARA MODELADO GP")
    print("=" * 70)
    
    # 1. Cargar datos
    filepath = CONFIG.DATA_CLEAN_PATH
    if not filepath.exists():
        print(f"‚ùå No se encontr√≥: {filepath}")
        print("   Ejecuta primero: mining-pipeline")
        return
    
    print(f"\nüìÇ Cargando: {filepath}")
    df = pd.read_csv(filepath, index_col=0, parse_dates=True, nrows=10000)  # Solo 10k para diagn√≥stico
    
    print(f"   Dimensiones: {df.shape}")
    print(f"   Rango temporal: {df.index.min()} ‚Üí {df.index.max()}")
    
    target = CONFIG.GP_TARGET_COLUMN
    
    # 2. Verificar target
    print(f"\nüéØ TARGET: {target}")
    print("-" * 50)
    
    if target not in df.columns:
        print(f"   ‚ùå Columna '{target}' NO ENCONTRADA")
        print(f"   Columnas disponibles: {df.columns.tolist()}")
        return
    
    y = df[target]
    print(f"   Min:    {y.min():.4f}")
    print(f"   Max:    {y.max():.4f}")
    print(f"   Mean:   {y.mean():.4f}")
    print(f"   Std:    {y.std():.4f}")
    print(f"   NaN:    {y.isna().sum()} ({y.isna().mean()*100:.2f}%)")
    print(f"   Zeros:  {(y == 0).sum()} ({(y == 0).mean()*100:.2f}%)")
    
    # 3. Verificar variabilidad del target
    print(f"\nüìà VARIABILIDAD DEL TARGET")
    print("-" * 50)
    
    cv = y.std() / y.mean() * 100  # Coeficiente de variaci√≥n
    print(f"   Coef. Variaci√≥n: {cv:.2f}%")
    
    if cv < 5:
        print("   ‚ö†Ô∏è  ALERTA: Variabilidad MUY BAJA")
        print("      El target casi no var√≠a - GP tendr√° dificultades")
    elif cv < 10:
        print("   ‚ö†Ô∏è  Variabilidad baja - considerar m√°s features")
    else:
        print("   ‚úÖ Variabilidad adecuada")
    
    # 4. Verificar autocorrelaci√≥n (series temporales)
    print(f"\nüîÑ AUTOCORRELACI√ìN TEMPORAL")
    print("-" * 50)
    
    autocorr_1 = y.autocorr(lag=1)
    autocorr_10 = y.autocorr(lag=10)
    autocorr_100 = y.autocorr(lag=100)
    
    print(f"   Lag 1:   {autocorr_1:.4f}")
    print(f"   Lag 10:  {autocorr_10:.4f}")
    print(f"   Lag 100: {autocorr_100:.4f}")
    
    if autocorr_1 > 0.95:
        print("   ‚ö†Ô∏è  ALERTA: Autocorrelaci√≥n MUY ALTA")
        print("      Los datos consecutivos son casi id√©nticos")
        print("      Considera: subsamplear cada N registros")
    
    # 5. Verificar features
    print(f"\nüìä AN√ÅLISIS DE FEATURES")
    print("-" * 50)
    
    features = df.drop(columns=[target, "_iron_concentrate"], errors='ignore')
    
    print(f"   Total features: {len(features.columns)}")
    
    # Features constantes
    constantes = []
    for col in features.columns:
        if features[col].std() < 1e-6:
            constantes.append(col)
    
    if constantes:
        print(f"   ‚ö†Ô∏è  Features CONSTANTES (eliminar): {constantes}")
    else:
        print("   ‚úÖ No hay features constantes")
    
    # Features con alta correlaci√≥n con target
    print(f"\n   Correlaci√≥n con target ({target}):")
    correlaciones = features.corrwith(y).abs().sort_values(ascending=False)
    
    for col, corr in correlaciones.head(10).items():
        emoji = "üü¢" if corr > 0.3 else "üü°" if corr > 0.1 else "üî¥"
        print(f"      {emoji} {col}: {corr:.4f}")
    
    # 6. Verificar multicolinealidad
    print(f"\nüîó MULTICOLINEALIDAD (Features correlacionados entre s√≠)")
    print("-" * 50)
    
    corr_matrix = features.corr().abs()
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    
    high_corr_pairs = []
    for col in upper.columns:
        for idx in upper.index:
            if upper.loc[idx, col] > 0.95:
                high_corr_pairs.append((idx, col, upper.loc[idx, col]))
    
    if high_corr_pairs:
        print(f"   ‚ö†Ô∏è  {len(high_corr_pairs)} pares con correlaci√≥n > 0.95:")
        for p1, p2, c in high_corr_pairs[:5]:
            print(f"      {p1} ‚Üî {p2}: {c:.4f}")
        print("   Considera eliminar features redundantes")
    else:
        print("   ‚úÖ No hay multicolinealidad extrema")
    
    # 7. Recomendaciones
    print(f"\nüí° RECOMENDACIONES")
    print("=" * 70)
    
    recomendaciones = []
    
    if autocorr_1 > 0.95:
        recomendaciones.append(
            "‚Ä¢ SUBSAMPLEAR: Toma cada 10-20 registros para reducir autocorrelaci√≥n"
        )
    
    if cv < 10:
        recomendaciones.append(
            "‚Ä¢ FEATURE ENGINEERING: Agregar lags, diferencias, o rolling stats"
        )
    
    if correlaciones.max() < 0.3:
        recomendaciones.append(
            "‚Ä¢ FEATURES D√âBILES: Ning√∫n feature tiene buena correlaci√≥n con target.\n"
            "  Considera: lags temporales, interacciones, transformaciones"
        )
    
    if len(high_corr_pairs) > 5:
        recomendaciones.append(
            "‚Ä¢ REDUCIR DIMENSIONALIDAD: PCA o eliminar features redundantes"
        )
    
    if not recomendaciones:
        print("‚úÖ Los datos parecen adecuados para modelado GP")
    else:
        for r in recomendaciones:
            print(r)
    
    # 8. Guardar gr√°fico de diagn√≥stico
    print(f"\nüìä Generando gr√°fico de diagn√≥stico...")
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # Serie temporal del target
    axes[0, 0].plot(df.index[:500], y.iloc[:500], 'b-', linewidth=0.5)
    axes[0, 0].set_title(f'Serie Temporal: {target} (primeros 500)')
    axes[0, 0].set_xlabel('Tiempo')
    axes[0, 0].set_ylabel('Valor')
    
    # Histograma del target
    axes[0, 1].hist(y, bins=50, color='steelblue', edgecolor='white')
    axes[0, 1].axvline(y.mean(), color='red', linestyle='--', label=f'Mean: {y.mean():.2f}')
    axes[0, 1].set_title(f'Distribuci√≥n: {target}')
    axes[0, 1].legend()
    
    # Autocorrelaci√≥n
    lags = range(1, 101)
    autocorrs = [y.autocorr(lag=l) for l in lags]
    axes[1, 0].bar(lags, autocorrs, color='steelblue', width=1)
    axes[1, 0].axhline(0.95, color='red', linestyle='--', label='Umbral 0.95')
    axes[1, 0].set_title('Autocorrelaci√≥n por Lag')
    axes[1, 0].set_xlabel('Lag')
    axes[1, 0].set_ylabel('Autocorrelaci√≥n')
    axes[1, 0].legend()
    
    # Top correlaciones con target
    top_corr = correlaciones.head(10)
    axes[1, 1].barh(top_corr.index, top_corr.values, color='steelblue')
    axes[1, 1].set_title(f'Top 10 Features Correlacionados con {target}')
    axes[1, 1].set_xlabel('|Correlaci√≥n|')
    
    plt.tight_layout()
    
    output_path = CONFIG.RESULTS_DIR / "diagnostico_datos.png"
    CONFIG.RESULTS_DIR.mkdir(parents=True, exist_ok=True)
    plt.savefig(output_path, dpi=150)
    plt.close()
    
    print(f"   Guardado: {output_path}")
    
    print("\n" + "=" * 70)
    print("üèÅ Diagn√≥stico completado")
    

if __name__ == "__main__":
    diagnosticar_datos()
