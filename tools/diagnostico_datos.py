"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MÃ³dulo: tools/diagnostico_datos.py
Proyecto: Arquitectura Minera 4.0
Autor: Juan Galaz (Refactorizado por Gemini)
VersiÃ³n: 1.2.1
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DESCRIPCIÃ“N:
    Realiza una auditorÃ­a estadÃ­stica de los datos preprocesados. Su objetivo es
    detectar patologÃ­as en los datos (multicolinealidad, baja varianza, 
    autocorrelaciÃ³n extrema) que degradan el desempeÃ±o de modelos de Procesos 
    Gaussianos (GP) y causan RÂ² negativos.

REQUISITOS:
    - Haber ejecutado el pipeline de limpieza (mining_clean.csv).
    - ConfiguraciÃ³n vÃ¡lida en settings.py.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import sys
from pathlib import Path

# Asegurar que el interprete encuentre el mÃ³dulo 'core' y 'config'
sys.path.insert(0, str(Path(__file__).parent.parent))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from config.settings import CONFIG


def diagnosticar_datos():
    """
    Ejecuta un flujo de diagnÃ³stico integral sobre el dataset de minerÃ­a.
    
    Analiza:
    1. Integridad: Presencia de NaNs y ceros sospechosos.
    2. Varianza: Si el target se mueve lo suficiente para ser predecible.
    3. Tiempo: Si existe fuga de datos por autocorrelaciÃ³n.
    4. Features: Correlaciones fuertes y redundancias (multicolinealidad).
    
    Genera un reporte en consola y un dashboard visual en la carpeta /results.
    """
    
    print("\n" + "â•"*70)
    print(f"ğŸ”¬ AUDITORÃA DE DATOS: PROYECTO MINERO 4.0 (v1.2.1)")
    print("â•"*70)
    
    # --- 1. CARGA DE DATOS ---
    filepath = CONFIG.DATA_CLEAN_PATH
    if not filepath.exists():
        print(f"âŒ ERROR: No se encontrÃ³ el dataset en: {filepath}")
        print("   AsegÃºrate de haber corrido el pipeline de procesamiento primero.")
        return
    
    print(f"ğŸ“‚ Analizando fuente: {filepath.name}")
    # Cargamos 10k registros: suficiente para estadÃ­stica descriptiva sin saturar RAM
    df = pd.read_csv(filepath, index_col=0, parse_dates=True, nrows=10000)
    
    target = CONFIG.GP_TARGET_COLUMN
    if target not in df.columns:
        print(f"âŒ ERROR: El target '{target}' no existe en el archivo limpio.")
        print(f"   Columnas disponibles: {df.columns.tolist()[:5]}...")
        return

    # --- 2. ANÃLISIS DEL TARGET (Variable Dependiente) ---
    print(f"\nğŸ¯ ANÃLISIS DEL OBJETIVO: {target}")
    print("-" * 50)
    
    y = df[target]
    mean_val = y.mean()
    std_val = y.std()
    cv = (std_val / mean_val) * 100 if mean_val != 0 else 0 # Coeficiente de VariaciÃ³n
    
    print(f"   â€¢ Rango: [{y.min():.3f} - {y.max():.3f}]")
    print(f"   â€¢ Coef. VariaciÃ³n: {cv:.2f}% (Varianza relativa a la media)")
    
    # Nota tÃ©cnica: Si CV < 5%, el modelo le costarÃ¡ distinguir seÃ±al de ruido
    if cv < 5:
        print("     âš ï¸  ALERTA: Target casi constante. R2 podrÃ­a ser muy bajo.")

    # --- 3. AUTOCORRELACIÃ“N (Fuga de InformaciÃ³n Temporal) ---
    print(f"\nğŸ”„ AUTOCORRELACIÃ“N (Lag Analysis)")
    print("-" * 50)
    
    # Calculamos la correlaciÃ³n del dato actual con el anterior (Lag 1)
    ac_1 = y.autocorr(lag=1)
    print(f"   â€¢ AutocorrelaciÃ³n Lag 1: {ac_1:.4f}")
    
    # Nota tÃ©cnica: Si ac_1 > 0.95, los datos son tan parecidos que el modelo
    # puede "hacer trampa" prediciendo simplemente el valor anterior.
    if ac_1 > 0.95:
        print(f"     âš ï¸  RECOMENDACIÃ“N: Sube el SUBSAMPLE_STEP (Actual: {CONFIG.DEFAULT_SUBSAMPLE_STEP})")

    # --- 4. ANÃLISIS DE FEATURES (Variables Independientes) ---
    print(f"\nğŸ“Š ANÃLISIS DE PREDICTORES (Features)")
    print("-" * 50)
    
    # CORRECCIÃ“N AUDITORÃA: Drop dinÃ¡mico basado en CONFIG
    features = df.drop(columns=[target], errors='ignore')
    
    # Buscar features que no aportan informaciÃ³n (desviaciÃ³n estÃ¡ndar ~ 0)
    constantes = [c for c in features.columns if features[c].std() < 1e-6]
    if constantes:
        print(f"   â€¢ âŒ Features constantes detectadas: {constantes}")
    
    # --- 5. MULTICOLINEALIDAD (Redundancia) ---
    # Si dos sensores miden lo mismo, confunden al Proceso Gaussiano
    corr_matrix = features.corr().abs()
    # Tomamos solo la parte superior de la matriz para evitar duplicados
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    redundantes = [col for col in upper.columns if any(upper[col] > 0.95)]
    
    if redundantes:
        print(f"   â€¢ âš ï¸  Features altamente redundantes (>0.95): {len(redundantes)}")
        print(f"        Sugerencia: Revisar {redundantes[:3]}...")

    # --- 6. GENERACIÃ“N DE DASHBOARD VISUAL ---
    print(f"\nğŸ¨ Generando dashboard de diagnÃ³stico...")
    
    
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    plt.suptitle(f"DiagnÃ³stico de Datos: {target}", fontsize=16, fontweight='bold')

    # Plot 1: Serie Temporal (Visualizar tendencia y outliers)
    axes[0, 0].plot(y.iloc[:1000], color='#1f77b4', linewidth=1)
    axes[0, 0].set_title("Serie Temporal (Muestra 1k)")
    axes[0, 0].grid(True, alpha=0.3)

    # Plot 2: DistribuciÃ³n (Chequear normalidad para GP)
    sns.histplot(y, kde=True, ax=axes[0, 1], color='green')
    axes[0, 1].set_title(f"DistribuciÃ³n de {target}")

    # Plot 3: Top Correlaciones (Â¿QuiÃ©n manda en el proceso?)
    top_corr = features.corrwith(y).abs().sort_values(ascending=False).head(10)
    top_corr.plot(kind='barh', ax=axes[1, 0], color='#ff7f0e')
    axes[1, 0].set_title("Top 10 Predictores (Importancia Lineal)")

    # Plot 4: Matriz de CorrelaciÃ³n tÃ©rmica
    sns.heatmap(features.iloc[:, :15].corr(), cmap='RdBu_r', center=0, ax=axes[1, 1], cbar=False)
    axes[1, 1].set_title("Mapa de Calor (Primeras 15 features)")

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    
    # Guardado seguro
    output_img = CONFIG.RESULTS_DIR / "diagnostico_profundo.png"
    plt.savefig(output_img, dpi=120)
    plt.close()
    
    print(f"âœ… Dashboard guardado en: {output_img}")
    print("\n" + "â•"*70)
    print("ğŸ DIAGNÃ“STICO FINALIZADO: Revisa las alertas arriba antes de entrenar.")


if __name__ == "__main__":
    # Si se ejecuta directamente, corremos el diagnÃ³stico
    try:
        diagnosticar_datos()
    except Exception as e:
        print(f"âŒ Error crÃ­tico durante el diagnÃ³stico: {str(e)}")
